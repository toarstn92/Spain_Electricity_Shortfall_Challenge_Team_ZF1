{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Regression Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "We {**TEAM ZF1**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: Spain Electricity Shortfall Challenge\n",
    "\n",
    "The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:\n",
    "\n",
    "- 1. analyse the supplied data;\n",
    "- 2. identify potential errors in the data and clean the existing data set;\n",
    "- 3. determine if additional features can be added to enrich the data set;\n",
    "- 4. build a model that is capable of forecasting the three hourly demand shortfalls;\n",
    "- 5. evaluate the accuracy of the best machine learning model;\n",
    "- 6. determine what features were most important in the model’s prediction decision, and\n",
    "- 7. explain the inner working of the model to a non-technical audience.\n",
    "\n",
    "Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:\n",
    "\n",
    "> In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.\n",
    " \n",
    "On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997462e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "import statsmodels.graphics.api as sga\n",
    "import statsmodels.formula.api as sfa\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# print multiple outputs in a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "# PARAMETER_CONSTANT = ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to load the data from the `df_train` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"df_train.csv\")\n",
    "print(f\"There are {df1.shape[0]} rows and {df1.shape[1]} columns\")\n",
    "df1.head(2)\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "# Remove unnecessary column(s)\n",
    "\n",
    "df_train = df1.drop(labels=\"Unnamed: 0\", axis=1)\n",
    "print(f\"There are {df_train.shape[0]} rows and {df_train.shape[1]} columns\")\n",
    "df_train.head(10).T\n",
    "print('', end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6001c-d353-4415-a4c1-07f9be96918a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.1. Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:37.824204Z",
     "start_time": "2021-06-28T08:52:37.811206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to describe variable (including mode and median)\n",
    "\n",
    "def describe(df):\n",
    "    d = {0:[df.mean(), df.median(), df.mode()[0]]}\n",
    "    dat = pd.DataFrame(data=d).rename(index={0: \"Mean\", 1: \"Median\", 2: \"Mode\"})\n",
    "    return pd.concat([df.describe(), dat])\n",
    "\n",
    "# Data comprehension\n",
    "\n",
    "print(f\"There are {df_train.isnull().sum().sum()} null values\")\n",
    "df_train.isnull().sum()\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "print(\"Description of Valencia_pressure\")\n",
    "describe(df_train[\"Valencia_pressure\"])\n",
    "\n",
    "# Deal with null containing column(s)\n",
    "\n",
    "df_train_clean = df_train.copy()\n",
    "df_train_clean[\"Valencia_pressure\"] = df_train_clean[\"Valencia_pressure\"].fillna(df_train_clean[\"Valencia_pressure\"].mode()[0])\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "print(f\"There are {df_train_clean.isnull().sum().sum()} null values after subtituting with the mode\")\n",
    "df_train_clean.isnull().sum()\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "print(\"Description of cleaned Valencia_pressure\")\n",
    "describe(df_train_clean[\"Valencia_pressure\"])\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "# Access column dtypes\n",
    "\n",
    "df_train_clean.info()\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "# Convert object dtypes to float\n",
    "\n",
    "df_train_clean[\"Valencia_wind_deg\"] = df_train_clean[\"Valencia_wind_deg\"].str.extract(\"(\\d+)\").astype(int)\n",
    "df_train_clean[\"Seville_pressure\"] = df_train_clean[\"Seville_pressure\"].str.extract(\"(\\d+)\").astype(int)\n",
    "df_train_clean[\"time\"] = pd.to_datetime(df_train_clean[\"time\"])\n",
    "print(\"Time, Valencia_wind_deg, Seville_pressure columns has been respectively converted to:\")\n",
    "print(df_train_clean[\"time\"].dtypes)\n",
    "print(df_train_clean[\"Valencia_wind_deg\"].dtypes)\n",
    "print(df_train_clean[\"Seville_pressure\"].dtypes)\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "# extract features from date\n",
    "\n",
    "df_train_clean[\"time_year\"] = df_train_clean[\"time\"].dt.year.astype(int)\n",
    "df_train_clean[\"time_month\"] = df_train_clean[\"time\"].dt.month.astype(int)\n",
    "df_train_clean[\"time_day\"] = df_train_clean[\"time\"].dt.day.astype(int)\n",
    "df_train_clean[\"time_hour\"] = df_train_clean[\"time\"].dt.hour.astype(int)\n",
    "df_train_clean[\"time_weekday\"] = df_train_clean[\"time\"].dt.weekday.astype(int) # Monday is 0 and Sunday is 6\n",
    "df_train_clean[\"time_weeknumber\"] = df_train_clean[\"time\"].dt.week.astype(int)\n",
    "\n",
    "# Sort columns and drop noise (\"time\")\n",
    "\n",
    "df_train_clean_sort = df_train_clean[sorted(df_train_clean)]\n",
    "df_train_clean_sort = df_train_clean_sort.drop(labels=\"time\", axis=1)\n",
    "df_train_clean_sort.info()\n",
    "print('', end=\"\\n\\n\")\n",
    "\n",
    "# Univariable non-Graphical Analysis\n",
    "\n",
    "print(f\"Univariable non-Graphical Analysis\")\n",
    "print(f\"There are {df_train_clean_sort.shape[0]} rows and {df_train_clean_sort.shape[1]} columns\")\n",
    "df_train_clean_sort.describe().T\n",
    "print(f\"There are 5 cities; Barcelona, Bilbao, Madrid, Seville, Valencia\", end=\"\\n\"\n",
    "      \"There are 5 variables reoccuring across all cities; Pressure, temp, temp_max, temp_min, wind_speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7353e6-1a44-420e-8dab-b02efa596380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns containing negative values\n",
    "\n",
    "df_train_clean_sort.columns[(df_train_clean_sort < 0).any()].tolist()\n",
    "df_train_clean_sort[(df_train_clean_sort[df_train_clean_sort.columns] < 0).any(axis=1)][['load_shortfall_3h']].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a3a339-99d1-4efe-88a7-c62c22774c0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2. Plot relevant feature interactions (correlation and linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97597187-eafa-4951-9800-0eaceed80d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to observe interactions\n",
    "\n",
    "y_0 = df_train_clean_sort[[\"load_shortfall_3h\"]]\n",
    "x_0 = df_train_clean_sort.drop(labels=\"load_shortfall_3h\", axis=1)\n",
    "\n",
    "y_0.head(2)\n",
    "x_0.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fc133-c877-47a2-8271-c67ebed36628",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.3. Investigate all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327c286-5a91-4360-bea2-154d3caa0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for linearity\n",
    "# Due to the number of visuals created, this function takes some time to run\n",
    "\n",
    "def scatter_plot(predictor, response, plotrow=1, plotcolumn=1, figsize=(4,3)):\n",
    "    fig, axs = plt.subplots(plotcolumn,plotrow, figsize=figsize)\n",
    "    fig.subplots_adjust(hspace = 0.5, wspace=.2)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for index, column in enumerate(predictor.columns):\n",
    "        axs[index].title.set_text(\"{} vs. Y\".format(column))\n",
    "        predictor_plots = axs[index].scatter(x=predictor[column],y=response, color=\"blue\", edgecolor=\"white\")\n",
    "\n",
    "    fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b0d08-40d5-44ec-90f0-af5351bbe41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for linearity between all predictor vairiables and y_0\n",
    "\n",
    "scatter_plot(x_0, y_0, plotrow=4, plotcolumn=13, figsize=(16,39))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd8fef-7173-4716-9a4f-86c9a0e4901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between predictor variables and response variable\n",
    "\n",
    "df_xy = x_0.join(y_0)\n",
    "\n",
    "# Function to get correlation coefficients and p-values of each x to y\n",
    "\n",
    "def p_values(df, y=\"load_shortfall_3h\", dec_place=6, p_value_threshold=0.1):\n",
    "    corrs = df.corr()[y]\n",
    "    dict_cp = {}\n",
    "\n",
    "    column_titles = [col for col in corrs.index if col!=y]\n",
    "    for col in column_titles:\n",
    "        p_val = round(pearsonr(df[col], df[y])[1],dec_place)\n",
    "        dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                        'P_Value':p_val}\n",
    "\n",
    "    df_cp = pd.DataFrame(dict_cp).T\n",
    "    df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "    return df_cp_sorted[df_cp_sorted['P_Value']<p_value_threshold]\n",
    "\n",
    "# Correlation and p-value of x and y\n",
    "\n",
    "p_values(df_xy, dec_place=6, p_value_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a782e-1497-4252-9d90-1d4c6789dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show predictor correlation heatmap and list columns with high correlation\n",
    "\n",
    "def corr_heatmap(corr, diag_len, corr_threshold):\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    with plt.rc_context():\n",
    "        plt.rc(\"figure\", figsize=(diag_len, diag_len))\n",
    "        predictor_corrs_fig = sns.heatmap(corr, mask=mask)\n",
    "\n",
    "    r, c = np.where(np.abs(corr) > corr_threshold)\n",
    "    off_diagonal = np.where(r != c)\n",
    "    corr_list = [row for row in corr.iloc[r[off_diagonal], c[off_diagonal]].index]\n",
    "    return corr_list\n",
    "\n",
    "# Show all predictors correlation heatmap and list columns with high correlation\n",
    "\n",
    "predictor_corrs = x_0.corr()\n",
    "corr_heatmap(predictor_corrs, diag_len=15, corr_threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7dbfd-ce1d-47f6-9e57-1ea97f49677c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.3. Investigate temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15355edb-8fb1-428c-919a-88722f79a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw time series (ts) plot\n",
    "\n",
    "def ts_plot(df, x, y, title=\"\", xlabel='Time', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "# Function to show y_0 time series (ts) plot\n",
    "    \n",
    "y_0_ts = df_train_clean[[\"time\"]].join(other=y_0[\"load_shortfall_3h\"])\n",
    "\n",
    "def y_0_plot(df=y_0_ts, x=y_0_ts.time, y=y_0_ts.load_shortfall_3h, title='tri-hourly load shortfall from 2015 to 2017', xlabel='Time', ylabel=\"Load shortfall\", dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099bc1c2-5446-4b34-9250-5f912eb95d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp = x_0.filter(regex=\"temp$\", axis=1)\n",
    "x_temp[\"Mean_temp\"] = x_temp.mean(axis=1)\n",
    "x_temp.head(2)\n",
    "x_temp.describe()\n",
    "scatter_plot(x_temp, y_0, plotrow=3, plotcolumn=2, figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc34e78-9772-4720-8e98-f47a7e004acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate temp_max variable\n",
    "\n",
    "x_temp_max = x_0.filter(regex=\"max$\", axis=1)\n",
    "x_temp_max[\"Mean_temp_max\"] = x_temp_max.mean(axis=1)\n",
    "x_temp_max.head(2)\n",
    "x_temp_max.describe()\n",
    "scatter_plot(x_temp_max, y_0, plotrow=3, plotcolumn=2, figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c19459-7931-4f92-b0e1-f3db6adcc24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate temp_min variable\n",
    "\n",
    "x_temp_min = x_0.filter(regex=\"min$\", axis=1)\n",
    "x_temp_min[\"Mean_temp_min\"] = x_temp_min.mean(axis=1)\n",
    "x_temp_min.head(2)\n",
    "x_temp_min.describe()\n",
    "scatter_plot(x_temp_min, y_0, plotrow=3, plotcolumn=2, figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b35d9d-2595-4434-a753-557eaf80eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate mean of all temp variables\n",
    "\n",
    "x_temp_mean = x_temp[[\"Mean_temp\"]].join(other = [x_temp_max[\"Mean_temp_max\"], x_temp_min[\"Mean_temp_min\"]])\n",
    "xy_temp_mean = x_temp_mean.join(y_0)\n",
    "xy_temp_mean.head(2)\n",
    "\n",
    "# Correlation and p-value of temperature predictor variables and y\n",
    "\n",
    "p_values(xy_temp_mean, y=\"load_shortfall_3h\", dec_place=6, p_value_threshold=0.1)\n",
    "\n",
    "# Correlation heatmap of temperature predictor variables\n",
    "\n",
    "temp_predictor_corr = x_temp_mean.corr()\n",
    "corr_heatmap(temp_predictor_corr, diag_len=3, corr_threshold=0.9)\n",
    "\n",
    "print('', end=\"\\n\")\n",
    "print(\"High correlation observed for all variables; using all of these variables will result in working with redundant information\")\n",
    "print(\"Choose one better correlated (and lower p-value) to the y_0: 'Mean_temp_min'\")\n",
    "print('', end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdb56b-1ea5-433a-81d4-6751f672c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp_min time series plot compared with y_0\n",
    "\n",
    "temp_min_ts = df_train_clean[[\"time\"]].join(other=x_temp_min[\"Mean_temp_min\"])\n",
    "temp_min_ts.head(2)\n",
    "ts_plot(temp_min_ts, x=temp_min_ts.time, y=temp_min_ts.Mean_temp_min, title='tri-hourly minimum temperature from 2015 to 2017', ylabel=\"Minimum temperature\")\n",
    "y_0_plot()\n",
    "print(\"Seasonality observed for temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65388f87-3c83-4d9f-b101-d3d49e537efe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.2. Investigate wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2484a63-11c0-4a25-8979-a2bb80e1de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wind_speed = x_0.filter(regex=\"speed$\", axis=1)\n",
    "x_wind_speed[\"Mean_wind_speed\"] = x_wind_speed.mean(axis=1)\n",
    "x_wind_speed.head(2)\n",
    "x_wind_speed.describe()\n",
    "scatter_plot(x_wind_speed, y_0, plotrow=3, plotcolumn=2, figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d2445-b1ce-4a6d-b93d-ce669ecaa079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind speed time series plot compared with y_0\n",
    "\n",
    "wind_speed_ts = df_train_clean[[\"time\"]].join(other=x_wind_speed[\"Mean_wind_speed\"])\n",
    "wind_speed_ts.head(2)\n",
    "ts_plot(wind_speed_ts, x=wind_speed_ts.time, y=wind_speed_ts.Mean_wind_speed, title='tri-hourly wind speed from 2015 to 2017', ylabel=\"Wind speed\")\n",
    "y_0_plot()\n",
    "print(\"Stationary time series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f3a2c-e654-44ce-b827-92a02938d4ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.3. Investigate wind degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc30d6-117e-4037-a010-93b4170ab334",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wind_deg = x_0.filter(regex=\"deg$\", axis=1)\n",
    "x_wind_deg[\"Mean_wind_deg\"] = x_wind_deg.mean(axis=1)\n",
    "x_wind_deg.head(2)\n",
    "x_wind_deg.describe()\n",
    "scatter_plot(x_wind_deg, y_0, plotrow=4, plotcolumn=1, figsize=(12,3))\n",
    "\n",
    "print(\"Valencia_wind_deg is observerd to consist extreme value range and seem to be a categorical variable\")\n",
    "print('', end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57352ea-6fb6-4fde-bfc7-07b1f0928e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for extreme values of wind_deg across cities\n",
    "\n",
    "x_wind_deg1 = x_0.filter(regex=r'(Barcelona_wind_deg|Bilbao_wind_deg)', axis=1)\n",
    "x_wind_deg1[\"Mean_wind_deg1\"] = x_wind_deg1.mean(axis=1)\n",
    "x_wind_deg1.head(2)\n",
    "x_wind_deg1.describe()\n",
    "scatter_plot(x_wind_deg1, y_0, plotrow=3, plotcolumn=1, figsize=(12,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c3fbe-9ef6-420b-8420-f588157b4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind_deg time series plot compared with y_0\n",
    "\n",
    "wind_deg_ts = df_train_clean[[\"time\"]].join(other=x_wind_deg1[\"Mean_wind_deg1\"])\n",
    "wind_deg_ts.head(2)\n",
    "ts_plot(wind_deg_ts, x=wind_deg_ts.time, y=wind_deg_ts.Mean_wind_deg1, title='tri-hourly wind degree (strength) from 2015 to 2017', ylabel=\"Wind degree\")\n",
    "y_0_plot()\n",
    "print(\"Partial seasonality observed for wind degree (strength)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933e423-aa45-4ce1-b51a-d8707db4d6f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.4. Investigate pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b83fb9-bb72-4f89-97f6-fae61499e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pressure = x_0.filter(regex=\"pressure$\", axis=1)\n",
    "x_pressure[\"Mean_pressure\"] = x_pressure.mean(axis=1)\n",
    "x_pressure.head(2)\n",
    "x_pressure.describe()\n",
    "scatter_plot(x_pressure, y_0, plotrow=3, plotcolumn=2, figsize=(12,6))\n",
    "\n",
    "print(\"Barcelona_pressure contributed heavily to the mean across cities due to its extreme value range\")\n",
    "print(\"Seville_pressure is also observerd to consist extreme value range and seem to be a categorical variable\")\n",
    "print('', end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd64c1d-c4ad-48ce-9205-8467535a069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for extreme values of pressure across cities\n",
    "\n",
    "x_pressure1 = x_0.filter(regex=r'(Bilbao_pressure|Madrid_pressure|Valencia_pressure)', axis=1)\n",
    "x_pressure1[\"Mean_pressure1\"] = x_pressure1.mean(axis=1)\n",
    "x_pressure1.head(2)\n",
    "x_pressure1.describe()\n",
    "scatter_plot(x_pressure1, y_0, plotrow=2, plotcolumn=2, figsize=(8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37dc1b2-7aee-450b-a10b-c6cde0e0ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pressure time series plot compared with y_0\n",
    "\n",
    "pressure_ts = df_train_clean[[\"time\"]].join(other=x_pressure1[\"Mean_pressure1\"])\n",
    "pressure_ts.head(2)\n",
    "ts_plot(pressure_ts, x=pressure_ts.time, y=pressure_ts.Mean_pressure1, title='tri-hourly pressure from 2015 to 2017', ylabel=\"Pressure\")\n",
    "y_0_plot()\n",
    "print(\"Nonconstant variance observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005aa1d3-63e6-4df6-af81-83861f421d14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.5. Investigate rain amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d5ca4-d7f7-4882-a969-24d13b74d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rain = x_0.filter(regex=\"rain\", axis=1)\n",
    "x_rain[\"Mean_rain\"] = x_rain.mean(axis=1)\n",
    "x_rain.head(2)\n",
    "x_rain.describe()\n",
    "scatter_plot(x_rain, y_0, plotrow=4, plotcolumn=2, figsize=(16,6))\n",
    "print(\"Due to the fact that our time series data span 3 hours interval, only rain variable in the same interval will be consedered\")\n",
    "print('', end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ced4df-194b-4297-a24b-39f7cbd066b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust for confromity with data time interval\n",
    "\n",
    "x_rain1 = x_0.filter(regex=\"rain_3h\", axis=1)\n",
    "x_rain1[\"Mean_rain_3h\"] = x_rain1.mean(axis=1)\n",
    "x_rain1.head(2)\n",
    "x_rain1.describe()\n",
    "scatter_plot(x_rain1, y_0, plotrow=3, plotcolumn=1, figsize=(12,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf143d-18fd-477b-955e-082b36e1bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rain_3h time series plot compared with y_0\n",
    "\n",
    "rain_3h_ts = df_train_clean[[\"time\"]].join(other=x_rain1[\"Mean_rain_3h\"])\n",
    "rain_3h_ts.head(2)\n",
    "ts_plot(rain_3h_ts, x=rain_3h_ts.time, y=rain_3h_ts.Mean_rain_3h, title='tri-hourly rain from 2015 to 2017', ylabel=\"Rain amount\")\n",
    "y_0_plot()\n",
    "print(\"Rain amount observed as white noise (white noise is completely random data with a mean of 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82755c3f-ee6f-4ced-bd2c-8f8dec44a58f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.6. Investigate humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ccd70-5482-4f19-8c46-140656925494",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_humidity = x_0.filter(regex=\"humidity\", axis=1)\n",
    "x_humidity[\"Mean_humidity\"] = x_humidity.mean(axis=1)\n",
    "x_humidity.head(2)\n",
    "x_humidity.describe()\n",
    "scatter_plot(x_humidity, y_0, plotrow=4, plotcolumn=1, figsize=(16,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591d392-634c-4620-b0da-9a3d03494bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Humidity time series plot compared with y_0\n",
    "\n",
    "humidity_ts = df_train_clean[[\"time\"]].join(other=x_humidity[\"Mean_humidity\"])\n",
    "humidity_ts.head(2)\n",
    "ts_plot(humidity_ts, x=humidity_ts.time, y=humidity_ts.Mean_humidity, title='tri-hourly humidity from 2015 to 2017', ylabel=\"Humidity\")\n",
    "y_0_plot()\n",
    "print(\"Seasonality observed for humidity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e23217-7235-40fb-8516-ad9c0ce323ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.7. Investigate level of cloud coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f656a-9902-44f6-ba64-c8dbf3b14db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clouds_all = x_0.filter(regex=\"clouds\", axis=1)\n",
    "x_clouds_all[\"Mean_clouds_all\"] = x_clouds_all.mean(axis=1)\n",
    "x_clouds_all.head(2)\n",
    "x_clouds_all.describe()\n",
    "scatter_plot(x_clouds_all, y_0, plotrow=4, plotcolumn=1, figsize=(16,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7f338-85fc-40b1-b8c1-be12a9579761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rain_3h time series plot compared with y_0\n",
    "\n",
    "clouds_all_ts = df_train_clean[[\"time\"]].join(other=x_clouds_all[\"Mean_clouds_all\"])\n",
    "clouds_all_ts.head(2)\n",
    "ts_plot(clouds_all_ts, x=clouds_all_ts.time, y=clouds_all_ts.Mean_clouds_all, title='tri-hourly cloud coverage from 2015 to 2017', ylabel=\"Cloud coverage\")\n",
    "y_0_plot()\n",
    "print(\"Partial seasonality observed for level of cloud coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698edf1d-4405-4ec0-9a10-4e91d55d801f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.8. Investigate snow amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bae2d9-89a2-45c6-944d-8a4f3625e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_snow_3h = x_0.filter(regex=\"snow\", axis=1)\n",
    "x_snow_3h[\"Mean_snow_3h\"] = x_snow_3h.mean(axis=1)\n",
    "x_snow_3h.head(2)\n",
    "x_snow_3h.describe()\n",
    "scatter_plot(x_snow_3h, y_0, plotrow=3, plotcolumn=1, figsize=(12,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560fa9c-5c9a-4458-9874-517ed014430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snow_3h time series plot compared with y_0\n",
    "\n",
    "snow_3h_ts = df_train_clean[[\"time\"]].join(other=x_snow_3h[\"Mean_snow_3h\"])\n",
    "snow_3h_ts.head(2)\n",
    "ts_plot(snow_3h_ts, x=snow_3h_ts.time, y=snow_3h_ts.Mean_snow_3h, title='tri-hourly snow amount from 2015 to 2017', ylabel=\"Snow amount\")\n",
    "y_0_plot()\n",
    "print(\"Snow amount observed as white noise (white noise is completely random data with a mean of 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451c856-e2b5-4597-be43-4053dff2b79f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2.9. Investigate weather id (weather condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67ea90-e1c9-40fc-9961-404e12004fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_weather_id = x_0.filter(regex=\"weather\", axis=1)\n",
    "x_weather_id[\"Mean_weather_id\"] = x_weather_id.mean(axis=1)\n",
    "x_weather_id.head(2)\n",
    "x_weather_id.describe()\n",
    "scatter_plot(x_weather_id, y_0, plotrow=3, plotcolumn=2, figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b75366-0fdb-46eb-98f6-70b5fec5d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snow_3h time series plot compared with y_0\n",
    "\n",
    "weather_id_ts = df_train_clean[[\"time\"]].join(other=x_weather_id[\"Mean_weather_id\"])\n",
    "weather_id_ts.head(2)\n",
    "ts_plot(weather_id_ts, x=weather_id_ts.time, y=weather_id_ts.Mean_weather_id, title='tri-hourly weather condition from 2015 to 2017', ylabel=\"Weather condition\")\n",
    "y_0_plot()\n",
    "print(\"Stationary time series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b97c5-7ebd-4185-9815-52e9d33a0bc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.3. Variable Selection based on observed Correlation and Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07bf657-29e0-41b1-9e9b-42836e0ce634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of varibles across cities\n",
    "\n",
    "print(\"Mean of varibles across cities\")\n",
    "x_mean = x_temp[[\"Mean_temp\"]].join(other = [x_temp_max[\"Mean_temp_max\"], x_temp_min[\"Mean_temp_min\"], x_wind_speed[\"Mean_wind_speed\"],\n",
    "                                             x_wind_deg[\"Mean_wind_deg\"], x_pressure[\"Mean_pressure\"], x_rain[\"Mean_rain\"], x_humidity[\"Mean_humidity\"],\n",
    "                                             x_clouds_all[\"Mean_clouds_all\"], x_snow_3h[\"Mean_snow_3h\"], x_weather_id[\"Mean_weather_id\"]])\n",
    "x_mean.head(2)\n",
    "x_mean.shape\n",
    "x_mean.describe().T\n",
    "\n",
    "# Mean of varibles (as adjusted during investigation) across cities\n",
    "\n",
    "print('', end=\"\\n\\n\")\n",
    "print(\"Varibles (as adjusted during investigation)\")\n",
    "x_mean1 = x_temp_min[[\"Mean_temp_min\"]].join(other = [x_wind_speed[\"Mean_wind_speed\"], x_wind_deg1[\"Mean_wind_deg1\"], x_pressure1[\"Mean_pressure1\"],\n",
    "                                                      x_rain1[\"Mean_rain_3h\"], x_humidity[\"Mean_humidity\"], x_clouds_all[\"Mean_clouds_all\"],\n",
    "                                                      x_snow_3h[\"Mean_snow_3h\"], x_weather_id[\"Mean_weather_id\"]])\n",
    "x_mean1.head(2)\n",
    "x_mean1.shape\n",
    "x_mean1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729dc9c-5c4e-4550-a0ae-6cadb9ffafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation and p-value of x and y\n",
    "\n",
    "x_mean_y = x_mean.join(y_0)\n",
    "p_values(x_mean_y, dec_place=6, p_value_threshold=0.1)\n",
    "\n",
    "x_mean1_y = x_mean1.join(y_0)\n",
    "p_values(x_mean1_y, dec_place=6, p_value_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce2a2b-9a6e-48cc-b177-486348002988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correlation heatmap and list columns with high correlation for x_mean\n",
    "\n",
    "mean_corrs = x_mean.corr()\n",
    "corr_heatmap(mean_corrs, diag_len=5, corr_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be168b-ebe7-4c26-811a-f386bcd0a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correlation heatmap and list columns with high correlation x_mean1\n",
    "\n",
    "mean1_corrs = x_mean1.corr()\n",
    "corr_heatmap(mean1_corrs, diag_len=4, corr_threshold=0.73)\n",
    "print(\"correlations between selected variables minimal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037cd25d-245e-4559-ba98-d58345917388",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.5. Investigate OLS fit summary using the various model dataframe so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9f790-76da-40e4-b918-75c95d74dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit model\n",
    "\n",
    "def fit_model(df, y=y_0):\n",
    "    df_fit = df.copy()\n",
    "    y_name = ''.join([col for col in y.columns])\n",
    "    X_name = [col for col in df_fit.columns]\n",
    "\n",
    "    # Build OLS formula string \" y ~ X \"\n",
    "\n",
    "    formula_str = y_name+\" ~ \"+\" + \".join(X_name)\n",
    "\n",
    "    model = sfa.ols(formula=formula_str, data=df_fit.join(y))\n",
    "    fitted = model.fit()\n",
    "    print(fitted.summary())\n",
    "    \n",
    "# Fit model of all x varible\n",
    "\n",
    "fit_model(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d2b30-754b-4961-98c4-034cc6513a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model using varible means across cities\n",
    "\n",
    "x_mean.shape\n",
    "fit_model(x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd768cff-4656-463b-a696-585f238b101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model using selected varible means across cities\n",
    "\n",
    "x_mean1.shape\n",
    "fit_model(x_mean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f36a694-8596-4991-ae72-8d6721a2d854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model of selected varible means with observed seasonality over time\n",
    "\n",
    "x_mean2 = x_mean1[[\"Mean_temp_min\", \"Mean_humidity\", \"Mean_clouds_all\", \"Mean_wind_deg1\"]].copy()\n",
    "x_mean2.head(2)\n",
    "\n",
    "fit_model(x_mean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa092925-9ce4-43d5-ae25-f6329d6c0827",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.6. Final file so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733664d-d6d2-41a0-ab5d-0737633febe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_for_use = x_mean1.copy()\n",
    "\n",
    "x_for_use.head(2)\n",
    "x_for_use.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f882b9ad-4ba6-4cd7-a829-7788db142b97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4. Variable Selection by Variance Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0b055-0e05-4a60-b0a5-81b97198440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f5ac3-bbee-4299-a51e-88945afaebcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing values/ features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59692724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to standardize the relevant dfs\n",
    "# predictors df= x_d1\n",
    "# dependent variable df= y_d1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler= StandardScaler()\n",
    "\n",
    "x_d1= x_0.copy() \n",
    "y_d1= y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create a scaled version of the predictors based on z score value\n",
    "x_d1_scaled= scaler.fit_transform(x_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_d1_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d1.to_csv('y_train.csv',float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std.to_csv('x_train.csv',float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert the scaled predictor variables into a dataframe\n",
    "\n",
    "x_std= pd.DataFrame(x_d1_scaled, columns=x_d1.columns)\n",
    "x_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3043b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regularisation\n",
    "# This is best after data scaling as regularisation a model for large coefficients\n",
    "# Import dependencies\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test, being sure to use the standardised predictors\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_std,\n",
    "                                                    y_d1,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e67612",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge= Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be89a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0= float(ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebe1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff= pd.DataFrame(ridge.coef_, X_train.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([x_d1_std.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8cc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ea2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "# creating a random forest model for the data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets and features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d073e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one or more ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model and motivate why it is the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss chosen methods logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
